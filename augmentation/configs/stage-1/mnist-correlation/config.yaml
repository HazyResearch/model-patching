# Configuring CycleGAN training
prefix: cyclegan
mode: train

# Weights and Biases
wandb_entity: hazy-research # change this to your entity
wandb_project: model-patching
wandb_group: stage-1
wandb_job_type: mnist-correlation

# Random seed
seed: 1

# Architecture
norm_type: batchnorm

# Replay buffer
replay_buffer_size: 50

# Optimizer
lr_gen: 0.001
lr_disc: 0.00005
beta_1_gen: 0.5
beta_1_disc: 0.5

# Training
n_epochs: 100000
batch_size: 25
gan_loss: bce
cycle_loss_scale: 10.0
identity_loss_scale: 1.0

# Dataset splitting
validation_frac: 0.0

# Dataset settings
source_dataset: mnist_correlation_partial/zigzag/0/200/y
source_dataset_version: 1.*.*
source_dataset_modifier: 'shuffle:1000,1'

target_dataset: mnist_correlation_partial/zigzag/1/200/y
target_dataset_version: 1.*.*
target_dataset_modifier: 'shuffle:1000,2'

datadir: /home/tfdata/ # adjust this path

# Dataflow settings
dataflow: disk_cached
cache_dir: /home/tfcache/ # adjust this path to a temporary cache for Tensorflow

# Training data augmentation settings
train_daug_pipeline: [HeuristicImageAugmentationPipeline, HeuristicImageAugmentationPipeline, BasicImagePreprocessingPipeline]
train_daug_pipeline_args: [['rotr'], ['pad:crop'], [minusone-one]]

# Validation data augmentation settings
val_daug_pipeline: [BasicImagePreprocessingPipeline]
val_daug_pipeline_args: [[minusone-one]]

# Test data augmentation settings
test_daug_pipeline: [BasicImagePreprocessingPipeline]
test_daug_pipeline_args: [[minusone-one]]

# Path to checkpoints in wandb.run directory
checkpoint_path: '/checkpoints/'

# Checkpoint every _ epochs
checkpoint_freq: 20

# Log images every _ steps
image_log_freq: 100

# Resuming an old run from Weights and Biases
resume: False
prev_wandb_run_id: null
prev_wandb_project: null
prev_wandb_entity: hazy-research
prev_ckpt_path: '' # where are the checkpoints located in wandb (relative to this run's root path)
